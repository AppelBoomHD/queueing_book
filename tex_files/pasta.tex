\section{Poisson Arrivals See Time Averages}
\label{sec:poisson-arrivals-see}


\opt{solutionfiles}{
\subsection*{Theory and Exercises}
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}

Suppose the following limit exists:
\begin{equation}\label{eq:jaap}
 \pi(n) 
= \lim_{m\to\infty} 
\frac1m\sum_{k=1}^m \1{L(A_k-) = n},
\end{equation}
then $\pi(n)$ is the long-run fraction of jobs that observe $n$ customers in the system at the moment an arbitrary job arrives.
It is natural to ask whether $\pi(n)$ and $p(n)$, as defined by~\cref{eq:p(n)}, are related, that is, whether what customers see upon arrival is related to the time-average behavior of the system.
In this section we will derive the famous \recall{Poisson arrivals see time averages} (\recall{PASTA}) condition that ensures that $\pi(n)=p(n)$ if jobs arrive in accordance with a Poisson process.


%We can make some progress by rewriting $\pi(n)$ in the following way.
Since $A(t)\to \infty$ as $t\to\infty$, it is reasonable that (see~\cref{ex:18} for a proof)
\begin{equation}\label{eq:132}
 \begin{split}
 \pi(n) &= \lim_{t\to\infty} \frac1{A(t)}\sum_{k=1}^{A(t)} \1{L(A_k-) = n} 
= \lim_{t\to\infty} \frac1{A(t)}\sum_{k=1}^\infty \1{A_k \leq t, L(A_k-) = n} \\
 &= \lim_{t\to\infty} \frac{A(n,t)}{A(t)},
 \end{split}
\end{equation}
where we use~\cref{eq:19} in the last row. But, with~\cref{eq:3}, 
\begin{equation}\label{eq:1333}
 \frac{A(n,t)}{t} 
= \frac{A(t)}t \frac{A(n,t)}{A(t)}
\to \lambda \pi(n), \quad\text{as } t \to \infty, 
\end{equation}
while by ~\cref{eq:21}, 
\begin{equation*}
\frac{A(n,t)}t = \frac{A(n,t)}{Y(n,t)}\frac{Y(n,t)}t \to \lambda(n) p(n), \quad\text{as } t \to \infty.
\end{equation*}
Thus
\begin{equation}\label{eq:13}
\lambda \pi(n) = \lambda(n) p(n).
\end{equation}
This leads to our final result:
\begin{equation*}
 \lambda(n) = \lambda \iff \pi(n) = p(n).
\end{equation*}
This means that if the arrival rate does not depend on the state of the system, i.e., $\lambda(n)=\lambda$, the sample probabilities $\{\pi(n)\}$ are equal to the time-average probabilities $\{p(n)\}$. 
In other words, the customer perception at arrival moments is the same as the server perception.

As the next exercises show, this property is not satisfied in general.
However, when the arrival process is Poisson we have that $\lambda(n)=\lambda$.
This fact is called \emph{PASTA: Poisson Arrivals See Time Averages}.
Thus, for the $M/M/1$ queue in particular,
\begin{equation*}
 \pi(n) = p(n) = (1-\rho)\rho^n.
\end{equation*}

\begin{exercise}\clabel{ex:8} 
Show for the case of~\cref{ex:112} that $\pi(0)=1$ and $\pi(n)=0$, for $n>0$.
\begin{solution}
  All arrivals see an empty system.
  Hence $A(0,t)/A(t) \approx (t/2)/(t/2) = 1$, and $A(n,t)=0$ for $n>0$.
  Thus, $\pi(0) = \lim_{t\to\infty} A(0,t)/A(t) = 1$ and $\pi(n)=0$ for $n>0$.
  Recall from the other exercises that $p(0)=1/2$.
  Hence, statistics as obtained via time averages are not necessarily the same as statistics obtained at arrival moments (or any other point process).
\end{solution}

\end{exercise}

\begin{exercise}\clabel{ex:l-152}
 Check that~\cref{eq:13} holds for the system of~\cref{ex:8}.
\begin{solution}
From the relevant previous exercises, $\lambda = \lim_{t\to\infty} A(t)/t = 1/2$. $\lambda(0)=1$, $p(0)=1/2$, and $\pi(0)=1$. Hence,
\begin{equation*}
 \lambda \pi(0) = \lambda(0) p(0) \implies \frac 1 2 \times 1 = 1\times \frac 1 2.
\end{equation*}
For $n>0$ it's easy, everything is 0.
\end{solution}
\end{exercise}





With the above reasoning, we can also establish a relation between $\pi(n)$ and the statistics of the system as obtained by the departures.
Define, analogous to~\cref{eq:132}, 
\begin{equation}
 \label{eq:33}
 \delta(n) = \lim_{t\to\infty} \frac{D(n,t)}{D(t)}
\end{equation}
as the long-run fraction of jobs that leave $n$ jobs \emph{behind}.
From~\cref{eq:15},
\begin{equation*}
\frac{A(t)}t \frac{A(n,t)}{A(t)} = \frac{A(n,t)}t \approx \frac{D(n,t)}t 
= \frac{D(t)}t \frac{D(n,t)}{D(t)}.
\end{equation*}
Taking limits at the left and right, and using~\cref{eq:28}, we obtain for (queueing) systems in which customers arrive and leave as single units that
\begin{equation}
 \label{eq:36}
 \lambda \pi(n) = \delta \delta(n).
\end{equation}
Thus, if the system is rate-stable and transitions occur one-by-one, the statistics obtained by arrivals is the same as statistics obtained by departures, i.e., 
\begin{equation}
 \label{eq:39}
\lambda = \delta \iff \pi(n) = \delta(n).
\end{equation}

\begin{exercise}\clabel{ex:pasta-26}
 For the $G/G/1$ queue, prove that the fraction of jobs that see $n$ jobs in the system upon arrival is the same as the fraction of departures that leave $n$ jobs behind.
 What condition have you used to prove this?
%\item Motivate that for the $G/M/1$, $\delta(n) = p(n)$, i.e., departures see time averages. 
\begin{hint}
Note that~\cref{eq:97}  does not depend on the distribution of the inter-arrival or service times.
\end{hint}
\begin{solution}
 All follows straightaway from the definitions in the main text.
 In the $G/G/1$ queue jobs arrive and depart in single units.
 Then, from~\cref{eq:97},
 \begin{equation*}
 \frac{A(t)}{t}\frac{A(n,t)}{A(t)} \approx 
 \frac{D(t)}{t}\frac{D(n,t)}{D(t)}. 
 \end{equation*}
 The left-hand side goes to $\lambda \pi(n)$ as $t\to\infty$, and
 the right-hand side to $\delta \delta(n)$. Use the fact that we
 always assume, implicitly, that the system is stable, so that
 $\lambda = \delta$. As a consequence $\delta(n) = \pi(n)$ for the $G/G/1$ queue.
\end{solution}
\end{exercise}


\begin{exercise}\clabel{ex:26}
 When $\lambda\neq \delta$, is $\pi(n)\geq \delta(n)$? 
\begin{hint}
 Use that $\lambda \geq \delta$ always holds. Thus, when $\lambda \neq \delta$, it must be that $\lambda > \delta$. What are the consequences of this inequality; how does the queue length behave as a function of time?
\end{hint}
\begin{solution}
 The assumptions lead us to conclude that $\lambda > \delta$. As a consequence, the queue length must increase in the long run (jobs come in faster than they leave). Therefore, $A(n,t)/t \to 0$ for all $n$, and also $D(n,t)/t\to 0$. Consequently, $\pi(n) = \delta(n) = 0$, which is the only sensible reconciliation with~\cref{eq:36}. 
\end{solution}
\end{exercise}

\begin{extra}\clabel{ex:909}
Show that 
\begin{equation*}
\lambda \pi(n) = \lambda(n) p(n) = \mu(n+1) p(n+1) = \delta \delta(n).
\end{equation*}
What is the important condition for this to be true?
\begin{hint}
Check all definitions of $Y(n,t)/t$ and so on.
\end{hint}
\begin{solution}
 The important condition is that transitions occur as single
 steps. In other words, the relation is true for processes with
 \recall{one-step transitions}, i.e., when $|A(n,t) - D(n,t)|\leq 1$.
 In that case, 
\begin{align*}
 \frac{A(n,t)}{t} &= \frac{A(n,t)}{A(t)} \frac{A(t)}{t} \to \pi(n) \lambda\\
 \frac{A(n,t)}{t} &= \frac{A(n,t)}{Y(n,t)} \frac{Y(n,t)}{t} \to \lambda(n)p(n)\\
 \frac{D(n,t)}{t} &= \frac{D(n,t)}{Y(n+1,t)} \frac{Y(n+1,t)}{t} \to \mu(n+1)p(n+1)\\
 \frac{D(n,t)}{t} &= \frac{D(n,t)}{D(t)} \frac{D(t)}{t} \to \delta(n)\delta. \\
\end{align*}
\end{solution}
\end{extra}

\begin{extra}\clabel{ex:58}
 Use PASTA and the balance equations of the $M/M/1$ queue to derive that $(\lambda + \mu) \pi(n) = \lambda \pi(n-1) + \mu \pi(n+1)$.
\begin{hint}
 Consider some state $n$ (not a level) and count all transitions that `go in and out of' this state.
 Specifically, $A(n,t) + D(n-1,t)$ counts all transitions out of state $n$: $A(n,t)$ counts the number of arrivals that see $n$ in the system upon arrival, hence immediately after such arrivals the system contains $n+1$ jobs; likewise, $D(n-1,t)$ counts all jobs that leave $n-1$ jobs behind, hence immediately before such jobs depart the system contains $n$ jobs.
 In a similar way, $A(n-1,t) + D(n,t)$ counts all transitions into state $n$ (Recall once again, $D(n,t)$ counts the jobs that leave $n$ behind.
 Hence, when such departures occur, state $n$ is entered).
 Now use that `what goes in must go out'.
\end{hint}
\begin{solution}
By the hint, the difference between the `out
 transitions' and the `in transitions' is at most 1 for all $t$. Thus, we can write
 \begin{align*}
\text{transitions out } &\approx \text{transitions in } \iff \\
 A(n,t) + D(n-1,t) &\approx A(n-1,t) + D(n,t) \iff \\
 \frac{A(n,t) + D(n-1,t)}t &\approx \frac{A(n-1,t) + D(n, t)}t \iff \\
 \frac{A(n,t)}t + \frac{D(n-1,t)}t &\approx \frac{A(n-1,t)}t + \frac{D(n,t)}t.
 \end{align*}
Using the ideas of~\cref{sec:level-cross-balance} this becomes for $t\to\infty$, 
\begin{equation*}
 (\lambda(n) +\mu(n))p(n) = \lambda(n-1)p(n-1) + \mu(n+1)p(n+1).
\end{equation*}
Since we are concerned here with the $M/M/1$ queue we have that
$ \lambda(n) = \lambda$ and $\mu(n) = \mu$, and using PASTA we have
that $p(n) = \pi(n)$. We are done.
\end{solution}
\end{extra}





\begin{exercise}\clabel{ex:18}
 There is a subtle problem in the transition from~\cref{eq:jaap} to~\cref{eq:132} and the derivation of~\cref{eq:1333}: $\pi(n)$ is defined as a limit over arrival epochs while in $A(n,t)/t$ we take the limit over time.
 Now the observant reader might ask why these limits should relate at all.
 Use the renewal reward theorem to show that~\cref{eq:132} is valid.
\begin{hint}
Check that the conditions of the renewal reward theorem are satisfied in the above proof of~\cref{eq:1333}. Then define 
\begin{align*}
 Y(t) &:= A(n,t) = \sum_{k=1}^{A(t)} \1{L(A_k-) = n} \\
X_k &:= Y(A_k) - Y(A_{k-1}) = A(n, A_k) - A(n, A_{k-1}) = \1{L(A_k-)=n}.
\end{align*}

\end{hint}
\begin{solution}
First we check the conditions. The counting process here is $\{A(t)\}$ and the epochs at which
 $A(t)$ increases are $\{A_k\}$. By assumption, $A_k\to\infty$,
 hence $A(t)\to\infty$ as $t\to\infty$. Moreover, by assumption
 $A(t)/t \to \lambda$. Also $A(n,t)$ is evidently non-decreasing and
 $A(n,t)\to\infty$ as $t\to\infty$.


From the definitions in the hint, 
\begin{equation*}
X= \lim_{m\to\infty} \frac 1 m \sum_{k=1}^m X_k =\lim_{m\to\infty} \frac 1 m \sum_{k=1}^m \1{L(A_k-)=n} = \pi(n).
\end{equation*}
Since $Y=\lim_{t\to\infty} Y(t)/t = \lim_{t\to\infty} A(n,t)/t$ it follows from the renewal reward theorem that
\begin{equation*}
 Y=\lambda X \implies \lim_{t\to\infty} \frac{A(n,t)} t = \lambda X = \lambda \pi(n).
\end{equation*}
Thus,~\cref{eq:1333} follows from the renewal reward theorem.
\end{solution}
\end{exercise}



\opt{solutionfiles}{
\Closesolutionfile{hint}
\Closesolutionfile{ans}
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}
%\clearpage


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../companion"
%%% End:
