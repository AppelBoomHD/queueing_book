\section{Open Single-Class Product-Form Networks}
\label{sec:jackson-networks}


\subsection*{Theory and Exercises}

\opt{solutionfiles}{
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}



With the PASTA property we can determine the distribution of the inter-departure times of the $M/M/1$ queue.
Observing that in a network of queues the departures from one queueing station form the arrivals at another station, we can use this result to analyze networks of queues




\begin{extra}\clabel{ex:dep}
Why is the output rate of the (stable) $M/M/1$ queue equal to~$\lambda$ and not~$\mu$?
\begin{solution}
Jobs arrive at rate $\lambda$. For a stable queue, $\mu>\lambda$. Moreover, jobs can never leave faster than they arrive.
\end{solution}
\end{extra}


\begin{extra}
 Why is $\mu e^{-\mu t}$ not a reasonable density for the inter-departure times?
 In fact, the simplest guess for the inter-departure density might be $\lambda e^{-\lambda t}$; so this is what we will try to prove below.
We will focus on departure moments and use~\cref{eq:39}, in particular that departures `see' what arrivals `see', i.e., $\delta(n)= \pi(n)$, and PASTA.
\begin{solution}
 Because jobs do not leave at rate $\mu$. 
\end{solution}
\end{extra}




\begin{extra}\clabel{ex:28}
Show that the probability that a job leaves behind a busy station is $\rho$, hence $1-\rho$ is the probability to leave an idle server behind.
\begin{solution}
Observe that $\rho$ is the fraction of time the server is busy. Then, from PASTA, the fraction of jobs that see a busy server is also $\rho$. This fraction of jobs is $\sum_{n=1}^\infty \pi(n)$. Finally, $\delta(n) = \pi(n)$ , a fraction $\rho$ of the departures leaves a busy system behind.

\end{solution}
\end{extra}


\begin{extra}\clabel{ex:17}
 If job $n-1$, say, leaves behind an empty system, show that the expected time until the next departure is $\E{D_n - D_{n-1}} = 1/\lambda + 1/\mu$. 
\begin{hint}
 After job $n-1$ left, job $n$ has to arrive, so we need to wait first for this inter-arrival time. Then job $n$ must be served. This adds up to $1/\lambda + 1/\mu$. 
\end{hint}
\begin{solution}
With the hint, we first have to wait for an inter-arrival
 time $X_n$. Then, since job $n$'s service starts right away, it
 leaves when $D_n = D_{n-1}+X_n + S_n$. Now observe that, due to the memoryless property of the inter-arrival times, $\E{X_n} = \E{A_n - D_{n-1}} = 1/\lambda$. Thus, the expected duration is $\E{X_n + S_n}=1/\lambda + 1/\mu$. 
\end{solution}
\end{extra}

\begin{extra}
Show that the density of $D_{n} - D_{n-1}$ is
 \begin{equation*}
 f_{X+S}(t) = \frac{\lambda \mu}{\lambda - \mu} (e^{-\mu t} - e^{-\lambda t})
 \end{equation*}
if the server is idle after $D_{n-1}$.
\begin{solution}
 By the previous point, the density of $D_{n} - D_{n-1}$ is the
 same as the density of $X_n + S_n$. Since $\{X_n\}$ and $\{S_n\}$ are both i.i.d. sequences, the problem becomes to find the density of $X+S$. We will use two ways of computing this. 

Since $X\sim \Exp(\lambda)$ and $S\sim\Exp(\mu)$, and $X$ and $S$ are independent, their joint density is $f_{X,S}(x,y) = \lambda \mu e^{-\lambda x - \mu y}$. With this,
 \begin{align*}
\P{X+S\leq t } 
&= \lambda \mu \int_0^\infty \int_0^\infty e^{-\lambda x - \mu y} \1{x+y\leq t} \d x \d y \\
&= \lambda \mu \int_0^t \int_0^{t-x} e^{-\lambda x - \mu y} \d y \d x \\
&= \lambda \mu \int_0^t e^{-\lambda x} \int_0^{t-x} e^{- \mu y} \d y \d x \\
&= \lambda \int_0^t e^{-\lambda x} (1-e^{- \mu (t-x)} ) \d x \\
&= \lambda \int_0^t e^{-\lambda x} \d x - \lambda e^{-\mu t} \int_0^t e^{(\mu-\lambda) x} \d x \\
&= 1- e^{-\lambda t} - \frac{\lambda}{\mu-\lambda} e^{-\mu t} ( e^{(\mu-\lambda) t} -1) \\
&= 1- e^{-\lambda t} - \frac{\lambda}{\mu-\lambda} e^{-\lambda t} + \frac{\lambda}{\mu-\lambda} e^{-\mu t} \\ 
&= 1 - \frac{\mu}{\mu-\lambda} e^{-\lambda t} + \frac{\lambda}{\mu-\lambda} e^{-\mu t}. \\
 \end{align*}
The density $f_{X+S}(t)$ is the derivative of this expression with respect to~$t$, hence,
\begin{align*}
 f_{X+S}(t) 
&= \frac{\lambda\mu}{\mu-\lambda} e^{-\lambda t} - \frac{\mu \lambda}{\mu-\lambda} e^{-\mu t} \\
&= \frac{\lambda\mu}{\lambda -\mu}(e^{-\mu t} - e^{-\lambda t}). \\
\end{align*}

Conditioning is much faster, but requires the concept of conditional density. You can skip the rest if you are not interested. 
 \begin{align*}
 f_{X+S}(t) 
&= \P{X+S\in \d{t}} \\
&= \int \P{S+x\in \d{t}}\P{X\in \d{x}} \\
&=\int_0^t f_S(t-x) f_X(x) \d{x} \\
 &= \int_0^t \mu e^{-\mu(t-x)} \lambda e^{-\lambda x} \d{x} \\
 &= \lambda \mu e^{-\mu t} \int_0^t e^{x(\mu-\lambda)} \d{x} \\
&= \frac{\lambda \mu}{\lambda - \mu}\left(e^{-\mu t} - e^{-\lambda t}\right).
 \end{align*}
\end{solution}
\end{extra}


\begin{extra}
Show that when the queue is not empty at a departure time, the density of the next inter-departure time is $f_D(t) = \mu e^{-\mu t}$.
\begin{solution}
After the departure, the server can start right away with the job at the head of the queue. The inter-departure time of this job is $\Exp(\mu)$.
\end{solution}
\end{extra}

\begin{extra}\clabel{ex:63}
Use conditioning on the server being idle or busy at a departure to show that the density of the inter-departure time is $\lambda e^{-\lambda t}$.
\begin{hint}
Conditioning leads to 
\begin{equation*}
 f_D(t) = f_{X+S}(t) \P{\text{server is idle}} + f_S(t) \P{\text{ server is busy }}= (1-\rho) f_{X+S}(t) +
 \rho \mu e^{-\mu t}.
\end{equation*}
 Now use the above exercises to simplify.
\end{hint}
\begin{solution}
 \begin{align*}
 f_D(t) 
&= (1-\rho) f_{X+S}(t) + \rho \mu e^{-\mu t} \\
&= (1-\rho) \frac{\mu\lambda}{\lambda-\mu} \left(e^{-\mu t}-e^{-\lambda t}\right) + \rho \mu e^{-\mu t} \\
&= \left(1-\frac{\lambda}\mu\right) \frac{\mu\lambda}{\lambda-\mu}\left(e^{-\mu t}-e^{-\lambda t}\right) + \rho \mu e^{-\mu t} \\
&= \frac{\mu-\lambda}\mu \frac{\mu\lambda}{\lambda-\mu}\left(e^{-\mu t}-e^{-\lambda t}\right) + \frac\lambda \mu \mu e^{-\mu t} \\
% &= \frac{\mu-\lambda}\mu \frac{\mu\lambda}{\lambda-\mu}\left(e^{-\mu t}-e^{-\lambda t}\right) + \lambda e^{-\mu t} \\
&= - \lambda\left(e^{-\mu t}-e^{-\lambda t}\right) + \lambda e^{-\mu t} \\
&= \lambda e^{-\lambda t}.
 \end{align*}
\end{solution}
\end{extra}


\begin{exercise}\clabel{ex:burke}
 Try to prove \recall{Burke's law} which states that the departure process of the $M/M/1$ queue is a Poisson process with rate $\lambda$.
\begin{solution}
 It follows from~\cref{ex:dep} to~\cref{ex:63} that inter-departures times have the same density, i.e., $\lambda e^{-\lambda t}$.
 It can also be shown that the inter-departure times are independent.

Thus, the inter-departures times form a set of i.i.d.
exponentially distributed random variables with mean $1/\lambda$.
Consequently, the departures times form a Poisson process with rate $\lambda$.

\end{solution}
\end{exercise}



The remark above Zijm.Eq.2.11 is not entirely correct. Remove the
sentence: `These visit ratios satisfy \ldots up to a multiplicative
constant'.


I don't like the derivation of Zijm.Eq.2.20. The appearance of the
visit ratios $\lambda_i/\gamma$ seems to come out of thin air. The
argument should be like this. Consider the entire queueing network as
one `box' in which jobs enter at rate $\gamma=\sum_{i=1}^M
\gamma_i$.
Assuming that there is sufficient capacity at each station, i.e.,
$\lambda_i < c_i \mu_i$ at each station $i$, the output rate of the `box' must also be $\gamma$. Thus, by applying Little's law to the `box', we have that 
\begin{equation*}
 \E L = \gamma \E W. 
\end{equation*}
It is also evident that the average total number of jobs must be equal
to the sum of the average number of jobs at each station: 
\begin{equation*}
 \E L = \sum_{i=1}^M \E{L_i}.
\end{equation*}
Applying Little's law to each station separately we get that
$\E{L_i} = \lambda_i\E{W_i}$. Filling this into the above,
\begin{equation*}
\E W = \frac{\E L}{\gamma} = \sum_{i=1}^M \frac{\E{L_i}}\gamma = \sum_{i=1}^M \frac{\lambda_i \E{ W_i}}\gamma, 
\end{equation*}
where we recognize the visit ratios.


\begin{exercise}[Linear algebra refresher]\clabel{ex:l-186} 
Can you find an example to
 show for two matrices $A$ and $B$ that $AB\neq BA$, hence
 $x A \neq A x$.
\begin{hint}
 Let 
 \begin{equation*}
A =
 \begin{pmatrix}
 1 & 1 \\ 
0&1
 \end{pmatrix},
\quad B=
 \begin{pmatrix}
 1 & 0 \\ 
1&1
 \end{pmatrix}.
 \end{equation*}
\end{hint}

\begin{solution}
 \begin{equation*}
 AB = 
 \begin{pmatrix}
 2 & 1 \\ 
1&1
 \end{pmatrix} 
\neq
 \begin{pmatrix}
 1 & 1 \\ 
1&2
 \end{pmatrix} 
= BA.
 \end{equation*}

Take $x=(1,1)$, then $x A=(1,2)$. Now, taking $x=
\begin{pmatrix}
 1 \\
1
\end{pmatrix}
$, we get $Ax = 
\begin{pmatrix}
 2 \\
1
\end{pmatrix}. $
Recall, horizontal vectors are not vertical vectors. The horizontal
ones are to the left of a matrix, and the vertical ones to the right.
\end{solution}
\end{exercise}

\begin{exercise}[Linear algebra refresher 2]\clabel{ex:l-187}
 Suppose the matrix $A$
 has an eigenvalue $0$. What is the geometric meaning of this fact? 
\begin{solution}
 Many students think that a matrix is just a bunch of numbers ordered in a grid.
 This is, in my opinion, the most unproductive way to think about matrices.
 A much more useful way is to see a matrix as an \emph{operator}.
 For instance, take $A$ to be a $3\times3$ matrix.
 Then it can be seen as a \emph{mapping} from $\R^3$ to $\R^3$; it takes a vector $x\in\R^3$ and changes $x$ into a new vector $Ax\in \R^3$.
 Thus, a square matrix $A$ typically changes the length and direction of a vector $x$.

 The next example is meant to illustrate what happens when a matrix has an eigenvalue 0. Consider the simple example with
 \begin{equation*}
A =
 \begin{pmatrix}
 1 & 0 & 0 \\ 
 0 & 1 & 0 \\ 
 0 & 0 & 0 \\ 
 \end{pmatrix}.
 \end{equation*}
Clearly, $A$ has an eigenvalue $0$. Now take $v=(x,y,z)$, so that
 \begin{equation*}
A v = A
 \begin{pmatrix}
x\\
y\\
z
 \end{pmatrix}
= \begin{pmatrix}
x\\
y\\
0
 \end{pmatrix}.
 \end{equation*}
 We see that $A$ removes any information about the $z$-direction from the vector $v$.
 (It projects $v$ on the $x-y$ plane, and throws away the $z$ component of $v$.)
 But then, for a given vector $w=(x,y,0)$ in the $x-y$ plane, it is impossible to use $A$ to retrieve the original vector $v=(x,y,z)$.
 Thus, $A$ cannot have an inverse on all of $\R^3$.

 So, hopefully, with this example, you can memorize that for any matrix $A$ to have an inverse, it is essential that it has no zero eigenvalues.
 When the \emph{operator} $A$ (don't think of a matrix as a set of numbers) throws away part of the dimension of the space on which it operates (i.e., it has one or more eigenvalue(s) $0$), it is impossible to retrieve the part of the space it throws away.
 Hence, its inverse cannot be used to get this part of the space back.

\end{solution}


\end{exercise}


\begin{exercise}\clabel{ex:l-188}
Zijm.Ex.2.2.1
\begin{solution}
Jobs arrive at rate $\lambda$. We always require rate-stability, i.e., $\lambda<\mu_1$. Thus, (unless jobs are created at the server (which is not the case)), the departure rate from station 1 must be $\lambda$. When there are jobs present at station 1, they leave at rate $\mu_1$. (Note that when there are no jobs at the station, there are no departures.) Likewise, the departure rate of station 2 is $\lambda$. 
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:l-189}
Zijm.Ex.2.2.2
\begin{solution}
 Observe from~\cref{ex:dep} that the inter-departure times of the $M/M/1$ queue are also independent and identically exponentially distributed with rate $\lambda$.
 Since the arrival process at the second station is the departure process of the first station, it must be that the arrival process at the second station is also Poisson with rate $\lambda$.
 Interestingly, from the perspective of the second station it is as if there is no first station.
\end{solution}
\end{exercise}



\begin{exercise}\clabel{ex:l-190}
Zijm.Ex.2.2.3 
\begin{solution}
 The question is not well specified. We know from Burke's law, see~\cref{ex:burke}, that
 the arrival \emph{process} at the second station is Poisson. If,
 however, we know that station 1 is empty, then it is unlikely that a
 job will arrive at station 2 in the very near future.

 Note that only the steady-state distributions of the queue lengths
 are independent. Once you have information about the state of one of
 the queues, then certainly this is not in `steady-state'.
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:l-191}
Zijm.Ex.2.2.4
\begin{solution}
 Simple algebra. (I am not going to write it out here. If you are
 willing to provide me the answer in \LaTeX\/ I'll include it.)
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:93}
 Zijm.Ex.2.2.5. The problem is not entirely correctly formulated. It
 should be, if for at least one $i$, $\sum_{j=1}^M P_{i j} <1$ \ldots
\begin{solution}
Linear algebra is quite useful here!

Observe that $P_{i j}$ is the probability that a job, after completing its service at node $i$, moves to node $j$.
Then $\sum_{j=1}^M P_{i j}$ is the probability that a job moves from node $i$ to another node in the network, i.e., stays in the network, while $P_{i0}$ is the probability that a job leaving node $i$ departs the network, in other words, the job is finished.
When $\sum_{j=1}^M P_{i j} < 1$, then more jobs enter node $i$ from the network than that node $i$ sends `back' into the network.
Conceptually, node $i$ `leaks jobs'.

Now, consider some node $k$ such that $P_{ki} > 0$, then the probability that a job that starts at node $k$, moves to node $i$ and then leaves the network is equal to $P_{ki}P_{i0}$.
Thus, since $P_{ki}>0$ and $P_{i0}>0$, the probability that a job leaves the network from node $k$ in two steps is positive.
More specifically, $P^2_{k0} = \sum_{j=0}^M P_{kj}P_{j0} \geq P_{ki}P_{i0} > 0$.

The irreducibility assumption implies that in at most $M$ steps it is possible to reach, with positive probability, any node from any other node in the network.
Thus, for any node $j$ to any other node $k$ there is a sequence of nodes $j_1, j_2, \ldots , j_{M-1}$ such that $P^{M}_{jk} \geq P_{j j_1}P_{j_1 j_2}\cdots P_{j_{M-1}k} > 0$.


Thus, if there is a node $i$ such that $P_{i0}>0$, then it is possible from any node that sends jobs to node $i$ directly to leave the network in two steps.
Likewise, when node $i$ can be reached from node $k$ in $n$ steps, say, then $P^{n+1}_{k0} \geq P^n_{ki}P_{i0} > 0$, i.e., in at most $n+1$ steps it is possible to leave the network from such node $k$.
This implies, in particular, that for all nodes $k=1,2,\ldots, M$, i.e., all nodes in the network, $P^{M+1}_{k0} >0$.
For this reason we consider $P^{M+1}$ in the hint.


As a final remark for students with knowledge of Markov chains, observe that the routing matrix $P$ does not correspond to the transition matrix of a recurrent Markov chain.
Since for at least one row $i$, $\sum_{j=1}^N P_{i j}<1$, the matrix $P$ is sub-stochastic.
Hence, a Markov chain induced by $P$ cannot be irreducible, because for this to happen, the chain must stay in some absorbing set with probability 1.
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:20}
Zijm.Ex.2.2.6
\begin{solution}
 Since $M$ is finite, and $k\leq M$, the set of numbers $P^{M+1}_{k0}$ is finite.
 This, together with the fact that $P^{M+1}_{k0}>0$ for all $k$, implies that there is some number $\epsilon>0$ such that $P^{M+1}_{k0}>\epsilon$.
 Hence, for all entries $k=1, 2, \ldots, M$, we have that $P^{M+1}_{kj} < 1- \epsilon$.
 This, in turn, implies that $P^{2(M+1)}_{kj} < (1- \epsilon)^2$, and so on, so that for any $n$, $P^{n(M+1)}_{kj} < (1- \epsilon)^n$.
 This implies, in more general terms, that the entries of $P^n$ decrease geometrically fast to $0$.

 It is well known that for any bounded sequence $x_i$ and $0\leq \alpha < 1$, $ \sum_{i=0}^\infty x_i \alpha^i < \infty$.
 By applying this insight to the entries of $P^n$ it follows that $\sum_{n=0}^\infty P^n_{jk} < \infty$.

Finally, applying $\lambda = \gamma + \lambda P$ recursively, we get
\begin{equation*}
 \lambda = \gamma + \lambda P = \gamma + (\gamma + \lambda P)P = \gamma (1+P) + \lambda P^2 = \gamma(1+P+P^2) + \lambda P^3 \to \gamma \sum_{n=0}^\infty P^n.
\end{equation*}
By the above reasoning, this last sum is well defined and finite.
(For math aficionados: the above argument is not necessarily valid for matrices $P$ that are infinite, since then $\inf\{P^{M}_{ik}\}$ need not be strictly positive.)

Another interesting way to see all this is by making the simplifying assumption that $P$ is a diagonalizable matrix.
(The argument can be generalized to include matrices reduced to Jordan normal form, but this gives optimal clutter, but does not change the line of reasoning in any fundamental way.)
In that case, there exists an invertible matrix $V$ with the (left) eigenvectors of $P$ as its rows and a diagonal matrix $\Lambda$ with the eigenvalues on its diagonal such that
\begin{equation*}
 V P = \Lambda V.
\end{equation*}
Hence, premultiplying with $V^{-1}$, 
\begin{equation*}
 P = V^{-1}\Lambda V.
\end{equation*}
But then
\begin{equation*}
P^2 = V^{-1}\Lambda V \cdot V^{-1}\Lambda V= V^{-1}\Lambda^2 V,
\end{equation*}
and in general $P^n = V^{-1}\Lambda^n V$.
If each eigenvalue $\lambda_i$ is such that its modulus $|\lambda_i| < 1$, then $\Lambda^n \to 0$ geometrically fast, hence $P^n\to 0$ geometrically fast, hence the sequence of partial sums $\sum_{n=0}^N P^n$ converges to a matrix with finite elements as $N\to\infty$.

So, we are left with proving that the eigenvalues of $P$ must have modulus less than $1$.
This fact follows from Gerschgorin's disk theorem, which I include for the interested student.
Define the disk $B(a,r)=\{z\in \mathbb{C} | |z-a|\leq r\}$, i.e., the set of complex numbers such that the distance to the center $a\in \mathbb{C} $ is less than or equal to the radius $r$.
With this, the Gerschgorin disks of a matrix are defined as $B(a_{ii}, \sum_{j\neq i} |a_{i j}|)$, i.e., disks with center at the diagonal elements $a_{ii}$ of $A$ and radius equal to the sum of the (modulus of the) elements of $A$ on the $i$th row except $a_{ii}$.
Then Gerschgorin's theorem says that all eigenvalues of $A$ lie in the union of these disks, i.e., all eigenvalues $\lambda_i \in \bigcup_i B({a_{ii}, \sum_{j\neq i}|a_{i j}})$.

Assume for notational simplicity that for each row $i$ of $P$ we have
that $\sum_{j} a_{i j}<1$. (Otherwise apply the argument to $P^{M+1}$.)
Then this implies for all $i$ that
\begin{equation*}
 a_{ii} + \sum_{j\neq i} a_{i j} < 1. 
\end{equation*}
Since all elements of $P$ are non-negative, this also implies that
\begin{equation*}
-1 < a_{ii} - \sum_{j\neq i} a_{i j} \leq a_{ii} + \sum_{j\neq i} a_{i j} < 1. 
\end{equation*}
With this and using that $a_{ii}$ is a real number (so that it lies on the real number axis) it follows that all elements in the disk $B(a_{ii}, \sum_{j\neq i} a_{i j})$ have modulus smaller than 1.
As this applies to any row $i$, all disks lie strictly within the complex unit circle.
But then, by Gerschgorin's theorem, all eigenvalues of $P$ also lie strictly in the unit circle, hence all eigenvalues have modulus smaller than 1.
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:23}
 Show that Zijm.Eq.2.13 and 2.14 can be written as
 \begin{equation*}
 f_i(n_i) = \frac{1}{G(i)}\frac{1}{\Pi_{k=1}^{n_i} \min\{k, c_i\}}\left( \frac{\lambda_i}{\mu_i}\right)^{n_i}.
 \end{equation*}
\begin{solution}
 Take $n_i<c_i$. Then
 $\Pi_{k=1}^{n_i} \min\{k, c_i\} = \Pi_{k=1}^{n_i} k = n_i!$, and
 $(c_i\rho_i)^{n_i} = (\lambda_i/\mu_i)^{n_i}.$ If $n_i\geq c_i$,
 then $\Pi_{k=1}^{n_i} \min\{k, c_i\} = c_i! c_i^{n_i-c_i}$, and
 $(c_i\rho_i)^{n_i} = (\lambda_i/\mu_i)^{n_i} c_i^{n_i}$.
\end{solution}
\end{exercise}


\begin{exercise}\clabel{ex:47}
 We have a two-station single-server open network.
 Jobs enter the network at the first station with rate $\gamma$.
 A fraction $\alpha$ returns from station 1 to itself; the rest moves to station 2. At station 2 a fraction $\beta_2$ returns to station 2 again, a fraction $\beta_1$ goes to station 1. Compute $\lambda$. What happens if $\alpha\to 1$ or $\beta_1\to 0$? 
\begin{solution}
 \begin{equation*}
 P = 
 \begin{pmatrix}
 \alpha & 1- \alpha \\
 \beta_1 & \beta_2
 \end{pmatrix}.
 \end{equation*}

 \begin{equation*}
 (\lambda_1, \lambda_2) = (\gamma, 0) + (\lambda_1, \lambda_2) P.
 \end{equation*}
Solving first for $\lambda_2$ leads to $\lambda_2 = (1-\alpha) \lambda_1 + \beta_2 \lambda_2$, so that 
\begin{equation*}
 \lambda_2 = \frac{1-\alpha}{1-\beta_2} \lambda_1. 
\end{equation*}
Next, using this and that $\lambda_1 = \alpha \lambda_1 + \beta_1 \lambda_2 + \gamma$ gives with a bit of algebra
\begin{equation*}
 \begin{split}
\gamma 
&= \lambda_1(1-\alpha) - \beta_1\lambda_2 \\
&= \lambda_1\left(1-\alpha - \beta_1\frac{1-\alpha}{1-\beta_2}\right) \\
&= \lambda_1(1-\alpha)\left(1 - \frac{\beta_1 }{1-\beta_2}\right) \\
&= \lambda_1(1-\alpha)\frac{1-\beta_1-\beta_2 }{1-\beta_2}.
 \end{split}
\end{equation*}
Hence, 
\begin{equation*}
 \lambda_1 = \frac\gamma{1-\alpha}\frac{1-\beta_2}{1-\beta_1-\beta_2}. 
\end{equation*}
Thus, 
\begin{equation*}
 \lambda_2 = \frac{1-\alpha}{1-\beta_2} \lambda_1 = \frac\gamma{1-\beta_1-\beta_2}. 
\end{equation*}


We want of course that $\lambda_1 < \mu_1$ and $\lambda_2 < \mu_2$.
With the above expressions this leads to conditions on $\alpha$, $\beta_1$ and $\beta_2$.
Note that we have three parameters, and two equations; there is not a single condition from which the stability can be guaranteed.

If $\alpha\uparrow 1$, the arrival rate at node $1$ explodes.
If $\beta_1=0$ no jobs are sent from node 2 to node 1.
\end{solution}
\end{exercise}

\begin{exercise}\clabel{ex:l-194}
Zijm.Ex.2.2.8
\begin{solution}
 Yes, the network remains a Jackson network. By Burke's law, see~\cref{ex:burke}, the
 departure process of each node is Poisson. In one of the earlier
 questions we derived that splitting (also known as thinning) and
 merging Poisson streams again lead to Poisson streams. The
 departures from node $j$ to node $k$ forms a thinned Poisson
 stream. The external arrivals plus internal arrivals are merged into
 one Poisson stream, hence the arrivals at a station also form a Poisson stream.

 Observe that the exponentiality of the service times and external
 inter-arrival times and Burke's law are essential for the argument.
\end{solution}
\end{exercise}





\opt{solutionfiles}{
\Closesolutionfile{hint}
\Closesolutionfile{ans}
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}
%\clearpage



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../companion"
%%% End:
